{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-task classification with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow-related\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from bert import optimization, tokenization, modeling\n",
    "# others\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from spellchecker import SpellChecker\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our re-implementations\n",
    "sys.path.insert(0, '../bert_reimplementations')\n",
    "import run_classifier_adapted as rc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "\n",
    "### Minor spell-checking\n",
    "We propose to apply two forms of word correction:\n",
    "1. We reduce contiguous repetitions of the same character to at most two consecutive repetitions. *Example: \"Loooose\" -> \"Loose\"*.\n",
    "2. We correct words' spelling if the correction requires changing at most two characters.\n",
    "3. Lower-case the data since that's how the pre-trained model was trained.\n",
    "\n",
    "In practice, the second takes very long as the comment texts can be pretty long. We've thus decided to omit this kind or preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce words' excessive character repetition\n",
    "def normalize_character_repetition(word):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", word)\n",
    "# correct a full sentence. We won't be using this one\n",
    "def sentence_correction(sentence, spellchecker):\n",
    "    sentence = \" \".join([\n",
    "        spellchecker.correction(normalize_character_repetition(word)) for word in sentence.split()\n",
    "        ])\n",
    "    return sentence\n",
    "# instantiate spellchecker object\n",
    "spellchecker = SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# original data\n",
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "# repetition-normalized\n",
    "train[\"comment_text\"] = train[\"comment_text\"].apply(lambda x: normalize_character_repetition(x.lower()))\n",
    "test[\"comment_text\"] = test[\"comment_text\"].apply(lambda x: normalize_character_repetition(x.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data split\n",
    "We'll leave out 10% of the training data as a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_VAL_RATIO = 0.9\n",
    "LEN = train.shape[0]\n",
    "SIZE_TRAIN = int(TRAIN_VAL_RATIO*LEN)\n",
    "\n",
    "# shuffle the train set\n",
    "indices = train.index.tolist()\n",
    "np.random.shuffle(indices)\n",
    "train = train.iloc[indices]\n",
    "# subset it\n",
    "val = train.iloc[SIZE_TRAIN:]\n",
    "train = train.iloc[:SIZE_TRAIN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT-style formatting\n",
    "The Google researchers' repository describes a specific format for us to feed our data to their model. This section takes care of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants similar to the Google repo\n",
    "ID = 'id'\n",
    "DATA_COLUMN = 'comment_text'\n",
    "LABEL_COLUMNS = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract training and test examples into bert-friendly objects\n",
    "label_names = train.columns.values.tolist()[2:]\n",
    "train_InputExamples = train.apply(lambda x: rc.InputExample(guid = x[\"id\"], text_a = x[\"comment_text\"],\n",
    "                                                            labels = x[label_names].tolist()), axis = 1)\n",
    "val_InputExamples = val.apply(lambda x: rc.InputExample(guid = x[\"id\"], text_a = x[\"comment_text\"],\n",
    "                                                        labels = x[label_names].tolist()), axis = 1)\n",
    "test_InputExamples = test.apply(lambda x: rc.InputExample(guid = x[\"id\"], text_a = x[\"comment_text\"],\n",
    "                                                          labels = [0]*6), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding\n",
    "The pre-trained model's vocabulary, checkpoint and model configuration are available for download [here](https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip?source=post_page---------------------------). The Google researchers' code uses a custom tokenizer to vectorize each word before feeding the data to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words to indices mapping\n",
    "BERT_VOCAB= \"../bert_pretrained/vocab.txt\"\n",
    "# pre-trained model weights\n",
    "BERT_INIT_CHKPNT = \"../bert_pretrained/bert_model.ckpt\"\n",
    "# BERT model architecture\n",
    "BERT_CONFIG = \"../bert_pretrained/bert_config.json\"\n",
    "\n",
    "# max sequence length\n",
    "MAX_SEQ_LENGTH = 500\n",
    "\n",
    "# tokenization\n",
    "tokenization.validate_case_matches_checkpoint(True, BERT_INIT_CHKPNT)\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file=BERT_VOCAB, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding\n",
    "train_features = rc.convert_examples_to_features(train_InputExamples, MAX_SEQ_LENGTH, tokenizer)\n",
    "val_features = rc.convert_examples_to_features(val_InputExamples, MAX_SEQ_LENGTH, tokenizer)\n",
    "test_features = rc.convert_examples_to_features(test_InputExamples, MAX_SEQ_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an input functions like the original repo\n",
    "train_input_fn = rc.input_fn_builder(features=train_features, seq_length=MAX_SEQ_LENGTH,\n",
    "                                                      is_training=True, drop_remainder=False)\n",
    "val_input_fn = rc.input_fn_builder(features=val_features, seq_length=MAX_SEQ_LENGTH,\n",
    "                                                      is_training=False, drop_remainder=False)\n",
    "test_input_fn = rc.input_fn_builder(features=test_features, seq_length=MAX_SEQ_LENGTH,\n",
    "                                                      is_training=False, drop_remainder=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the researchers copied these hyperparams from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "# then I copied them from the researchers\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 1.0\n",
    "# Warmup is a period of time where hte learning rate \n",
    "# is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Compute # train and warmup steps from batch size\n",
    "num_train_steps = int(len(train_examples) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 1000\n",
    "SAVE_SUMMARY_STEPS = 500\n",
    "OUTPUT_DIR = \"../bert_reimplementations/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify output directory and number of checkpoint steps to save\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "    keep_checkpoint_max=1,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = modeling.BertConfig.from_json_file(BERT_CONFIG)\n",
    "model_fn = model_fn_builder(\n",
    "  bert_config=bert_config,\n",
    "  num_labels= len(LABEL_COLUMNS),\n",
    "  init_checkpoint=BERT_INIT_CHKPNT,\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps,\n",
    "  use_tpu=False,\n",
    "  use_one_hot_embeddings=False\n",
    ")\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  config=run_config,\n",
    "  params={\"batch_size\": BATCH_SIZE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
